{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba612f3c-a5e3-47e3-acc0-a75af452cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ENTITY = \"confopt-team\"\n",
    "PROJECT = \"ConfoptAutoML25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be4323-cb8c-4466-a638-c5eeb6cddddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_run_data(run_id, entity=\"confopt-team\", project=\"ConfoptAutoML25\"):\n",
    "    \"\"\"\n",
    "    Pull data from a W&B run given the entity, project, and run_id.\n",
    "    \n",
    "    Args:\n",
    "        entity (str): The W&B entity (username or team).\n",
    "        project (str): The name of the W&B project.\n",
    "        run_id (str): The run identifier.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the run summary, config, and history (as a pandas DataFrame).\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    run_path = f\"{entity}/{project}/{run_id}\"\n",
    "    try:\n",
    "        run = api.run(run_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving run: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Retrieve summary metrics (as a dict)\n",
    "    summary = run.summary._json_dict\n",
    "\n",
    "    # Retrieve configuration used for the run\n",
    "    config = run.config\n",
    "\n",
    "    # Retrieve run history (logged metrics) as a pandas DataFrame\n",
    "    history = run.history(pandas=True)\n",
    "\n",
    "    return {\"summary\": summary, \"config\": config, \"history\": history}\n",
    "\n",
    "def get_run_ids_with_filter(filter_dict, entity=\"confopt-team\", project=\"ConfoptAutoML25\"):\n",
    "    \"\"\"\n",
    "    Retrieve the run IDs for all runs in a project that match the given filter.\n",
    "\n",
    "    Args:\n",
    "        entity (str): The W&B entity (username or team).\n",
    "        project (str): The name of the W&B project.\n",
    "        filter_dict (dict): A dictionary defining the filter criteria.\n",
    "            For example: {\"config.learning_rate\": 0.001}\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of run IDs that match the filter.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(f\"{entity}/{project}\", filters=filter_dict)\n",
    "    run_ids = [run.id for run in runs]\n",
    "    run_names = [run.name for run in runs]\n",
    "    return run_ids, run_names\n",
    "\n",
    "def print_wandb_links(run_ids, run_names, entity=ENTITY, project=PROJECT):\n",
    "    for run_id, run_name in zip(run_ids, run_names):\n",
    "        print(f\"https://wandb.ai/{entity}/{project}/runs/{run_id}/overview ({run_name} {run_id})\")\n",
    "\n",
    "def get_best_run_data(run_ids, last_epoch):\n",
    "    all_data = {}\n",
    "    best_run = None\n",
    "    best_genotype = None\n",
    "    best_loss = np.inf\n",
    "    losses = []\n",
    "    seeds = []\n",
    "\n",
    "    for run_id in run_ids:\n",
    "        data = pull_run_data(run_id)\n",
    "        all_data[run_id] = data\n",
    "\n",
    "        seed = data[\"config\"][\"trainer\"][\"seed\"]\n",
    "\n",
    "        assert seed not in seeds, f\"Duplicate seed {seed} found in run {run_id}\"\n",
    "        last_step = data[\"summary\"][\"_step\"] \n",
    "        assert last_step == last_epoch, f\"Last step {last_step} is not {last_epoch}\"\n",
    "\n",
    "        eval_loss = data[\"summary\"][\"eval/loss\"]\n",
    "        losses.append(eval_loss)\n",
    "    \n",
    "        if eval_loss < best_loss:\n",
    "            best_loss = eval_loss\n",
    "            best_run = run_id\n",
    "\n",
    "        seeds.append(seed)\n",
    "\n",
    "    print(losses, run_ids)\n",
    "    \n",
    "    return all_data[best_run]\n",
    "\n",
    "def get_run_ids_and_names(sampler, subspace, opset, dataset, batch_size, other=None, tag=\"first-full-run\"):\n",
    "    filter_dict = {\n",
    "        \"config.benchmark\": f\"{subspace}-{opset}\",\n",
    "        \"config.dataset\": dataset,\n",
    "        \"config.sampler_type\": sampler,\n",
    "        \"config.trainer.batch_size\": batch_size,\n",
    "        \"config.is_debug_run\": False,\n",
    "        \"config.oles.oles\": False,\n",
    "        \"config.partial_connector\": {\"$in\": [None]},\n",
    "        \"config.perturbator\": {\"$in\": [None]},\n",
    "        \"config.sampler.arch_combine_fn\": \"default\",\n",
    "        \"config.tag\": tag,\n",
    "        \"state\": \"finished\",\n",
    "    }\n",
    "\n",
    "    if other is not None:\n",
    "        if other == \"oles\":\n",
    "            filter_dict[\"config.oles.oles\"] = True\n",
    "        elif other == \"pcdarts\":\n",
    "            filter_dict[\"config.partial_connector\"] = {\"$nin\": [None]}\n",
    "        elif other == \"sdarts\":\n",
    "            filter_dict[\"config.perturbator\"] = {\"$nin\": [None]}\n",
    "        elif other == \"fairdarts\":\n",
    "            filter_dict[\"config.sampler.arch_combine_fn\"] = \"sigmoid\"\n",
    "\n",
    "    print(filter_dict)\n",
    "    \n",
    "    return get_run_ids_with_filter(filter_dict)\n",
    "\n",
    "batch_sizes = {\n",
    "    \"darts\": {\n",
    "        \"deep\": 64,\n",
    "        \"wide\": 96,\n",
    "        \"single_cell\": 96,\n",
    "    },\n",
    "    \"drnas\": {\n",
    "        \"deep\": 64,\n",
    "        \"wide\": 96,\n",
    "        \"single_cell\": 96,\n",
    "    },\n",
    "    \"gdas\": {\n",
    "        \"deep\": 320,\n",
    "        \"wide\": 480,\n",
    "        \"single_cell\": 480,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890ad27-2d95-4f97-a989-44c1e1803504",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar10_supernet\"\n",
    "\n",
    "best_genotypes = {}\n",
    "runs_without_results = []\n",
    "incomplete_runs = []\n",
    "\n",
    "samplers = (\"darts\", \"drnas\", \"gdas\")\n",
    "subspaces = (\"deep\", \"wide\", \"single_cell\")\n",
    "opsets = (\"regular\", \"all_skip\", \"no_skip\")\n",
    "darts_others = (None, \"oles\", \"pcdarts\", \"sdarts\", \"fairdarts\")\n",
    "\n",
    "for sampler in samplers:\n",
    "    for subspace in subspaces:\n",
    "        for opset in opsets:\n",
    "\n",
    "            batch_size = batch_sizes[sampler][subspace]\n",
    "            others = darts_others if sampler == \"darts\" else (None,)\n",
    "\n",
    "            for other in others:\n",
    "                run_ids, run_names = get_run_ids_and_names(\n",
    "                    sampler,\n",
    "                    subspace,\n",
    "                    opset,\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    other=other,\n",
    "                    tag=\"first-full-run\"\n",
    "                )\n",
    "\n",
    "                if len(run_ids) == 0:\n",
    "                    print(\"No results for: \", (sampler, subspace, opset, other))\n",
    "                    runs_without_results.append((sampler, subspace, opset, other))\n",
    "                    continue\n",
    "                elif len(run_ids) < 3:\n",
    "                    print(\"Fewer than 3 runs for: \", (sampler, subspace, opset, other))\n",
    "                    incomplete_runs.append((sampler, subspace, opset, other))\n",
    "                    continue\n",
    "\n",
    "                print(\"\\n\", sampler, subspace, opset, \"\" if other is None else other)\n",
    "                print_wandb_links(run_ids, run_names)\n",
    "                last_epoch = 99 if sampler in (\"darts\", \"drnas\") else 299\n",
    "\n",
    "                try:\n",
    "                    best_run_data = get_best_run_data(run_ids, last_epoch)\n",
    "                    best_genotype = best_run_data[\"summary\"][\"genotype\"]\n",
    "                    other_str = \"-baseline\" if other is None else f\"-{other}\"\n",
    "                    best_genotypes[f\"{sampler}{other_str}-{subspace}-{opset}\"] = best_genotype\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Assertion error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d64d1-f9bf-4bfe-a9e5-591ea748f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir genotypes\n",
    "\n",
    "for exp, genotype in best_genotypes.items():\n",
    "    with open(f\"genotypes/{exp}.txt\", \"w\") as f:\n",
    "        f.write(genotype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04999af7-5602-48c7-bd7d-ae28e62c18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_without_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4b2d3-ff25-4ff6-a638-43e9edd52087",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ef254",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_genotypes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(folder_path):\n",
    "    return sorted([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./genotypes\"\n",
    "files = list_files(folder_path)\n",
    "# print(files)\n",
    "# print(len(files))\n",
    "\n",
    "\n",
    "pieces = [(*f.split(\".\")[0].split(\"-\"),) for f in files]\n",
    "pieces\n",
    "\n",
    "hpsets = \"0,1,2,3,4\"\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "print(\"#!/bin/bash\\n\")\n",
    "for opt, other, subspace, opset in pieces:\n",
    "    print(f\"python launch_model_train.py --optimizer {opt} --subspace {subspace} --opset {opset} --dataset cifar10_model --hpsets {hpsets} --seed 0 --epochs {epochs} --other {other} --tag models-train --genotypes_folder exp/genotypes & sleep 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57f669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
