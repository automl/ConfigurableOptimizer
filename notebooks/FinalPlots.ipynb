{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare\n",
    "from cd_plot import cd_evaluation\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from autorank._util import RankResult, get_sorted_rank_groups, rank_multiple_nonparametric, test_normality\n",
    "\n",
    "\n",
    "HP_IDX = \"HP_Idx\"\n",
    "OPTIMIZER = \"Optimizer\"\n",
    "SUBSPACE = \"Subspace\"\n",
    "OPSET = \"Opset\"\n",
    "TEST_ACC1 = \"Test Accuracy\"\n",
    "BENCHMARK = \"Benchmark\"\n",
    "\n",
    "WIDE = \"wide\"\n",
    "DEEP = \"deep\"\n",
    "SINGLE_CELL = \"single_cell\"\n",
    "ALL_SKIP = \"all_skip\"\n",
    "NO_SKIP = \"no_skip\"\n",
    "REGULAR = \"regular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Plotting CD Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_cd_diagram(result, reverse, ax, width):\n",
    "    \"\"\"\n",
    "    !TAKEN FROM AUTORANK WITH MODIFICATIONS!\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_line(line, color=\"k\", **kwargs):\n",
    "        ax.plot([pos[0] / width for pos in line], [pos[1] / height for pos in line], color=color, **kwargs)\n",
    "\n",
    "    def plot_text(x, y, s, *args, **kwargs):\n",
    "        ax.text(x / width, y / height, s, *args, **kwargs)\n",
    "\n",
    "    sorted_ranks, names, groups = get_sorted_rank_groups(result, reverse)\n",
    "    cd = result.cd\n",
    "\n",
    "    lowv = min(1, int(math.floor(min(sorted_ranks))))\n",
    "    highv = max(len(sorted_ranks), int(math.ceil(max(sorted_ranks))))\n",
    "    cline = 0.4\n",
    "    textspace = 1\n",
    "    scalewidth = width - 2 * textspace\n",
    "\n",
    "    def rankpos(rank):\n",
    "        if not reverse:\n",
    "            relative_rank = rank - lowv\n",
    "        else:\n",
    "            relative_rank = highv - rank\n",
    "        return textspace + scalewidth / (highv - lowv) * relative_rank\n",
    "\n",
    "    linesblank = 0.2 + 0.2 + (len(groups) - 1) * 0.1\n",
    "\n",
    "    # add scale\n",
    "    distanceh = 0.1\n",
    "    cline += distanceh\n",
    "\n",
    "    # calculate height needed height of an image\n",
    "    minnotsignificant = max(2 * 0.2, linesblank)\n",
    "    height = cline + ((len(sorted_ranks) + 1) / 2) * 0.2 + minnotsignificant\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(width, height))\n",
    "        fig.set_facecolor(\"white\")\n",
    "        ax = fig.add_axes([0, 0, 1, 1])  # reverse y axis\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Upper left corner is (0,0).\n",
    "    ax.plot([0, 1], [0, 1], c=\"w\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)\n",
    "\n",
    "    plot_line([(textspace, cline), (width - textspace, cline)], linewidth=0.7)\n",
    "\n",
    "    bigtick = 0.1\n",
    "    smalltick = 0.05\n",
    "\n",
    "    tick = None\n",
    "    for a in list(np.arange(lowv, highv, 0.5)) + [highv]:\n",
    "        tick = smalltick\n",
    "        if a == int(a):\n",
    "            tick = bigtick\n",
    "        plot_line([(rankpos(a), cline - tick / 2), (rankpos(a), cline)], linewidth=0.7)\n",
    "\n",
    "    for a in range(lowv, highv + 1):\n",
    "        plot_text(rankpos(a), cline - tick / 2 - 0.05, str(a), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    for i in range(math.ceil(len(sorted_ranks) / 2)):\n",
    "        chei = cline + minnotsignificant + i * 0.2\n",
    "        plot_line([(rankpos(sorted_ranks[i]), cline), (rankpos(sorted_ranks[i]), chei), (textspace - 0.1, chei)], linewidth=0.7)\n",
    "        plot_text(textspace - 0.2, chei, names[i], ha=\"right\", va=\"center\")\n",
    "\n",
    "    for i in range(math.ceil(len(sorted_ranks) / 2), len(sorted_ranks)):\n",
    "        chei = cline + minnotsignificant + (len(sorted_ranks) - i - 1) * 0.2\n",
    "        plot_line([(rankpos(sorted_ranks[i]), cline), (rankpos(sorted_ranks[i]), chei), (textspace + scalewidth + 0.1, chei)], linewidth=0.7)\n",
    "        plot_text(textspace + scalewidth + 0.2, chei, names[i], ha=\"left\", va=\"center\")\n",
    "\n",
    "    # upper scale\n",
    "    if not reverse:\n",
    "        begin, end = rankpos(lowv), rankpos(lowv + cd)\n",
    "    else:\n",
    "        begin, end = rankpos(highv), rankpos(highv - cd)\n",
    "    distanceh += 0.15\n",
    "    bigtick /= 2\n",
    "    plot_line([(begin, distanceh), (end, distanceh)], linewidth=0.7)\n",
    "    plot_line([(begin, distanceh + bigtick / 2), (begin, distanceh - bigtick / 2)], linewidth=0.7)\n",
    "    plot_line([(end, distanceh + bigtick / 2), (end, distanceh - bigtick / 2)], linewidth=0.7)\n",
    "    plot_text((begin + end) / 2, distanceh - 0.05, \"CD\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # no-significance lines\n",
    "    side = 0.05\n",
    "    no_sig_height = 0.1\n",
    "    start = cline + 0.2\n",
    "    for l, r in groups:\n",
    "        plot_line([(rankpos(sorted_ranks[l]) - side, start), (rankpos(sorted_ranks[r]) + side, start)], linewidth=2.5)\n",
    "        start += no_sig_height\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def cd_evaluation(performance_per_dataset, maximize_metric, output_path=None, ignore_non_significance=False, plt_title=None):\n",
    "    \"\"\"Performance per dataset is  a dataframe that stores the performance (with respect to a metric) for  set of\n",
    "    configurations / models / algorithms per dataset. In  detail, the columns are individual configurations.\n",
    "    rows are datasets and a cell is the performance of the configuration for  dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # -- Preprocess data for autorank\n",
    "    if maximize_metric:\n",
    "        rank_data = performance_per_dataset.copy() * -1\n",
    "    else:\n",
    "        rank_data = performance_per_dataset.copy()\n",
    "    rank_data = rank_data.reset_index(drop=True)\n",
    "    rank_data = pd.DataFrame(rank_data.values, columns=list(rank_data))\n",
    "\n",
    "    # -- Settings for autorank\n",
    "    alpha = 0.05\n",
    "    effect_size = None\n",
    "    verbose = True\n",
    "    order = \"ascending\"  # always due to the preprocessing\n",
    "    alpha_normality = alpha / len(rank_data.columns)\n",
    "    all_normal, pvals_shapiro = test_normality(rank_data, alpha_normality, verbose)\n",
    "\n",
    "    # -- Friedman-Nemenyi\n",
    "    res = rank_multiple_nonparametric(rank_data, alpha, verbose, all_normal, order, effect_size, None)\n",
    "\n",
    "    result = RankResult(\n",
    "        res.rankdf,\n",
    "        res.pvalue,\n",
    "        res.cd,\n",
    "        res.omnibus,\n",
    "        res.posthoc,\n",
    "        all_normal,\n",
    "        pvals_shapiro,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        alpha,\n",
    "        alpha_normality,\n",
    "        len(rank_data),\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        res.effect_size,\n",
    "        None,\n",
    "    )\n",
    "    print(res.rankdf)\n",
    "    if result.pvalue >= result.alpha:\n",
    "        if ignore_non_significance:\n",
    "            warnings.warn(\"Result is not significant and results of the plot may be misleading!\")\n",
    "        else:\n",
    "            raise ValueError(\"Result is not significant and results of the plot may be misleading. If you still want to see the CD plot, set\" +\n",
    "                             \" ignore_non_significance to True.\")\n",
    "\n",
    "    # -- Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    plt.rcParams.update({\"font.size\": 16})\n",
    "    _custom_cd_diagram(result, order == \"ascending\", ax, 8)\n",
    "    if plt_title:\n",
    "        plt.title(plt_title)\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, transparent=True, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and make it pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and make it pretty\n",
    "df = pd.read_csv(\"model_trains_full.csv\")  # Update with your filename\n",
    "df[TEST_ACC1] = df[\"TestAcc1\"]\n",
    "df[OPTIMIZER] = df[OPTIMIZER].replace(\n",
    "    {\n",
    "        \"drnas\": \"DrNAS\",\n",
    "        \"darts\": \"DARTS\",\n",
    "        \"pcdarts\": \"PC-DARTS\",\n",
    "        \"sdarts\": \"SmoothDARTS\",\n",
    "        \"gdas\": \"GDAS\",\n",
    "        \"oles\": \"OLES\",\n",
    "        \"fairdarts\": \"FairDARTS\",\n",
    "    }\n",
    ")\n",
    "df[SUBSPACE] = df[SUBSPACE].replace(\n",
    "    {\n",
    "        \"wide\": \"Wide\",\n",
    "        \"deep\": \"Deep\",\n",
    "        \"single_cell\": \"Single Cell\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df[OPSET] = df[OPSET].replace(\n",
    "    {\n",
    "        \"all_skip\": \"All Skip\",\n",
    "        \"no_skip\": \"No Skip\",\n",
    "        \"regular\": \"Regular\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df[BENCHMARK] = df[SUBSPACE] + \" + \" + df[OPSET]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(df, by=\"Benchmark\"):\n",
    "    # Create a benchmark column for easier handling\n",
    "   \n",
    "    # Aggregate statistics\n",
    "    summary = df.groupby([by, \"Optimizer\"])[\"Test Accuracy\"].agg(\n",
    "        mean_accuracy=\"mean\",\n",
    "        std_accuracy=\"std\",\n",
    "        median_accuracy=\"median\",\n",
    "        min_accuracy=\"min\",\n",
    "        max_accuracy=\"max\"\n",
    "    ).reset_index()\n",
    "\n",
    "    print(summary.head())\n",
    "\n",
    "    # Plot boxplots for each benchmark\n",
    "    benchmarks = sorted(df[by].unique())\n",
    "\n",
    "    # Create a grid of subplots for all benchmarks\n",
    "    num_benchmarks = len(benchmarks)\n",
    "    cols = 3  # Number of columns in the grid\n",
    "    rows = (num_benchmarks + cols - 1) // cols  # Calculate the number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows), squeeze=True)\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, benchmark in enumerate(benchmarks):\n",
    "        ax = axes[i]\n",
    "        optimizers_order = sorted(df[\"Optimizer\"].unique())\n",
    "        sns.boxplot(\n",
    "            data=df[df[by] == benchmark],\n",
    "            x=\"Optimizer\",\n",
    "            y=\"Test Accuracy\",\n",
    "            order=optimizers_order,\n",
    "            ax=ax,\n",
    "            palette=\"Set2\"  # Assign different colors to each optimizer\n",
    "        )\n",
    "        ax.set_title(f\"{benchmark}\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.set_xlabel(\"Optimizer\")\n",
    "        ax.set_ylabel(\"Test Accuracy\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/boxplot_{by}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Box plot for all benchmarks\n",
    "box_plot(df, by=BENCHMARK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of the friedmanchisquare API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friedman_example():\n",
    "    rng = np.random.default_rng()\n",
    "    x = rng.random((9, 7))\n",
    "    # x = np.arange(100).reshape(-1, 10).tolist()\n",
    "    # print(x.shape)\n",
    "    res = friedmanchisquare(*x)\n",
    "    return res.statistic, res.pvalue\n",
    "\n",
    "for i in range(10):\n",
    "    print(friedman_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Friedman statistics for every benchmark and display it as a LateX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benchmark_rows(df, benchmark):\n",
    "    return df[df[BENCHMARK] == benchmark].reset_index(drop=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for benchmark in df[BENCHMARK].unique():\n",
    "    rows = get_benchmark_rows(df, benchmark).pivot(index=HP_IDX, columns=OPTIMIZER, values=TEST_ACC1)\n",
    "    values = rows.to_numpy().tolist()\n",
    "    res = friedmanchisquare(*values)\n",
    "    results.append((benchmark, res.statistic, res.pvalue))\n",
    "\n",
    "friedman_results = pd.DataFrame(results, columns=[\"Benchmark\", \"Friedman Stat\", \"p-value\"])\n",
    "print(friedman_results.to_latex(index=False, float_format=\"%.8f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the maximum test accuracy for every optimizer x benchmark combination and plot the CD diagram\n",
    "This should yield 9 benchmarks x 7 optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MAX test accuracy for each optimizer and benchmark and pivot the dataframe\n",
    "# to have optimizers as columns and benchmarks as rows\n",
    "grouped_df = df.groupby([OPTIMIZER, BENCHMARK])[TEST_ACC1].max().reset_index()\n",
    "pivot_df = grouped_df.pivot(index=BENCHMARK, columns=OPTIMIZER, values=TEST_ACC1)\n",
    "\n",
    "cd_evaluation(pivot_df, maximize_metric=True, output_path=\"plots/cd_plot.jpg\", ignore_non_significance=False)\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best HP Id per Optimizer x Benchmark combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[df.groupby([OPTIMIZER, BENCHMARK])[TEST_ACC1].idxmax()].groupby([BENCHMARK, OPTIMIZER])[HP_IDX].mean().reset_index().to_csv(\"best_hp_for_opt_and_benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df, method='spearman', figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Plot correlation heatmap between optimizers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with benchmarks as rows and optimizers as columns\n",
    "    method : str, default='spearman'\n",
    "        Correlation method ('spearman' or 'pearson')\n",
    "    figsize : tuple, default=(12, 10)\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    # Calculate correlation between optimizers (columns)\n",
    "    corr = df.corr(method=method)\n",
    "    \n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(\n",
    "        corr, \n",
    "        # mask=mask,\n",
    "        cmap=\"viridis\",\n",
    "        vmax=1.0,\n",
    "        vmin=-1.0,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=.5,\n",
    "        cbar_kws={\"shrink\": .5,}, # \"label\": f\"{method.capitalize()} Correlation\"},\n",
    "        annot=True,  # Show correlation values\n",
    "        fmt=\".2f\",    # Round to 2 decimal places\n",
    "        # fontsize=50,\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"{method.capitalize()} Rank Correlation Between Benchmarks\", fontsize=25)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(\"plots/correlation_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    return corr\n",
    "\n",
    "# Example usage:\n",
    "# corr_spearman = plot_correlation_heatmap(pivot_df.transpose(), method='spearman')\n",
    "corr_pearson = plot_correlation_heatmap(pivot_df.transpose(), method='kendall')\n",
    "pivot_df.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the Win Rate matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty Condorcet matrix\n",
    "optimizers = pivot_df.columns\n",
    "condorcet = pd.DataFrame(0, index=optimizers, columns=optimizers, dtype=int)\n",
    "\n",
    "# Loop through all pairs of optimizers\n",
    "for i in optimizers:\n",
    "    for j in optimizers:\n",
    "        if i != j:\n",
    "            condorcet.loc[i, j] = (pivot_df[i] > pivot_df[j]).mean()\n",
    "\n",
    "print(\"Condorcet matrix (raw wins):\")\n",
    "condorcet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(condorcet, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar_kws={\"shrink\": 0.5})\n",
    "plt.title(\"Win Rate\", fontsize=20)\n",
    "# plt.xlabel(\"Optimizer\")\n",
    "# plt.ylabel(\"Benchmark\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/condorcet_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
